# MoE
MoE模型全称是**混合专家模型**（Mixture of Experts, MoE），其主要将多个专家神经网络模型组合成一个更大的模型。  
MoE模型的核心组成有两部分：  
&emsp; 第一部分是**多个专家网络模型**，每个专家网络模型往往是独立的，且分别用于不同的问题；  
&emsp; 第二部分是**门控网络**，用于确定使用哪些专家网络模型，一般通过计算每个专家网络的分数（权重）来实现。
